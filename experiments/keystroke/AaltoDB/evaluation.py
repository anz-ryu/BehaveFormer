import torch
import torch.nn as nn
import torch.nn.functional as F
import torch

import onnx
import onnxruntime

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3)
        self.conv2 = nn.Conv2d(32, 64, 3)
        self.conv3 = nn.Conv2d(64, 64, 3)
        self.fc1 = nn.Linear(64*3*3, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        x = F.relu(F.max_pool2d(self.conv3(x), 2))
        x = x.view(-1, 64*3*3)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


model = torch.load('model1.pt', map_location=torch.device('cpu'))

print(type(model))  # dictなのか、state_dictなのか確認
print(model.keys() if isinstance(model, dict) else "Not a dictionary")

# モデルの入力形状を確認
def print_model_input_shape():
    if isinstance(model, dict):
        net = Net()
        net.load_state_dict(model)
    else:
        net = model
    
    # モデルの全体構造を表示
    print("モデル構造：")
    print(net)
    
    # モデルの全パラメータを取得
    print("\nモデルパラメータ：")
    for name, param in net.named_parameters():
        print(f"{name}: {param.shape}")

# print_model_input_shape()

def evaluate_with_random_input():
    # 評価モードに設定
    model.eval()
    
    # ランダム入力の作成
    batch_size = 1
    seq_length = 50 
    feature_dim = 10  
    
    # ランダム入力の生成
    random_input = torch.randn(batch_size, seq_length, feature_dim)
    
    print(f"\nランダム入力の形: {random_input.shape}")
    
    # no_gradを使用して推論
    with torch.no_grad():
        try:
            output = model(random_input)
            print(f"出力の形: {output.shape}")
            print(f"出力サンプル:\n{output}")
        except Exception as e:
            print(f"実行時エラー: {str(e)}")

# evaluate_with_random_input()

# ランダム入力の形: torch.Size([1, 50, 10])
# 出力の形: torch.Size([1, 64])

def convert_to_onnx():
    # モデルを評価モードに設定
    model.eval()
    
    # KeystrokeTransformerモデル用の入力形状
    batch_size = 1
    seq_length = 50 
    feature_dim = 10
    dummy_input = torch.randn(batch_size, seq_length, feature_dim)
    
    # ONNXモデルへの変換
    try:
        torch.onnx.export(model,                    # モデル
                         dummy_input,               # モデル入力例
                         "model.onnx",             # 保存先ファイル名
                         export_params=True,        # 学習済みパラメータを保存
                         opset_version=13,         # ONNXのバージョン
                         do_constant_folding=True,  # 定数畳み込みの最適化
                         input_names=['input'],     # 入力名
                         output_names=['output'],   # 出力名
                         dynamic_axes={'input': {0: 'batch_size', 1: 'sequence'},    # 可変サイズの次元
                                     'output': {0: 'batch_size'}})
        print("モデルをONNX形式に変換し、'model.onnx'として保存しました")
    except Exception as e:
        print(f"変換中にエラーが発生しました: {str(e)}")

def verify_onnx_model():
    
    try:
        # ONNXモデルの基本的な検証
        onnx_model = onnx.load("model.onnx")
        onnx.checker.check_model(onnx_model)
        print("ONNXモデルの構造は有効です")
        
        # ONNXランタイムセッションの作成
        ort_session = onnxruntime.InferenceSession("model.onnx")
        
        # テスト入力の作成（KeystrokeTransformer用）
        dummy_input = torch.randn(1, 50, 10)  # batch_size, seq_length, feature_dim
        input_name = ort_session.get_inputs()[0].name
        
        # ONNXモデルで推論実行
        ort_inputs = {input_name: dummy_input.numpy()}
        ort_output = ort_session.run(None, ort_inputs)
        
        # PyTorchモデルで同じ入力での推論実行
        model.eval()
        with torch.no_grad():
            torch_output = model(dummy_input)
        
        # 出力の比較
        print("\nPyTorch出力とONNX出力の比較:")
        print(f"PyTorch出力形状: {torch_output.shape}")
        print(f"ONNX出力形状: {ort_output[0].shape}")
        
        # 出力値の差異を確認
        import numpy as np
        np.testing.assert_allclose(torch_output.numpy(), 
                                 ort_output[0], 
                                 rtol=1e-03, 
                                 atol=1e-05)
        print("PyTorchとONNXの出力が一致しています！")
        
    except Exception as e:
        print(f"検証中にエラーが発生しました: {str(e)}")

# まずONNXに変換
print("ONNXへの変換を開始します...")
convert_to_onnx()

# 変換されたモデルを検証
print("\nONNXモデルの検証を開始します...")
verify_onnx_model()

def evaluate_with_real_data():
    # テストデータの準備（すべてのデータをフラットに配置）
    test_data = [[
        [0.09900000, 0.34299999, 0.44200000, 0.46799999, 0.56699997, 0.64399999, 0.74299997, 0.73500001, 0.83399999, 0.11764706],
        [0.12500000, 0.17600000, 0.30100000, 0.26699999, 0.39199999, 0.59299999, 0.71799999, 0.69300002, 0.81800002, 0.14509805],
        [0.09100000, 0.32600001, 0.41700000, 0.42600000, 0.51700002, 0.75300002, 0.84399998, 0.86900002, 0.95999998, 0.13725491],
        [0.10000000, 0.32699999, 0.42699999, 0.44299999, 0.54299998, 0.60399997, 0.70400000, 0.71899998, 0.81900001, 0.24313726],
        [0.11600000, 0.16100000, 0.27700001, 0.27599999, 0.39199999, 0.79400003, 0.91000003, 0.90300000, 1.01900005, 0.18039216],
        [0.11500000, 0.51800001, 0.63300002, 0.62699997, 0.74199998, 0.88599998, 1.00100005, 0.98500001, 1.10000002, 0.16862746],
        [0.10900000, 0.25900000, 0.36800000, 0.35800001, 0.46700001, 0.94400001, 1.05299997, 1.05200005, 1.16100001, 0.11372549],
        [0.09900000, 0.58600003, 0.68500000, 0.69400001, 0.79299998, 2.36599994, 2.46499991, 2.50699997, 2.60599995, 0.12549020],
        [0.10800000, 1.67200005, 1.77999997, 1.81299996, 1.92100000, 2.36500001, 2.47300005, 2.46399999, 2.57200003, 0.24313726],
        [0.14100000, 0.55199999, 0.69300002, 0.65100002, 0.79200000, 0.91100001, 1.05200005, 1.00199997, 1.14300001, 0.11372549],
        [0.09900000, 0.25999999, 0.35900000, 0.35100001, 0.44999999, 0.70200002, 0.80100000, 0.82700002, 0.92600000, 0.16470589],
        [0.09100000, 0.35100001, 0.44200000, 0.47600001, 0.56699997, 2.29099989, 2.38199997, 2.38899994, 2.48000002, 0.12549020],
        [0.12500000, 1.81500006, 1.94000006, 1.91299999, 2.03800011, 2.48300004, 2.60800004, 2.57500005, 2.70000005, 0.24313726],
        [0.09800000, 0.56999999, 0.66799998, 0.66200000, 0.75999999, 0.86199999, 0.95999998, 0.96899998, 1.06700003, 0.18431373],
        [0.09200000, 0.20000000, 0.29200000, 0.30700001, 0.39899999, 0.76800001, 0.86000001, 0.89300001, 0.98500001, 0.11372549],
        [0.10700000, 0.46100000, 0.56800002, 0.58600003, 0.69300002, 1.23000002, 1.33700001, 1.32900000, 1.43599999, 0.20000000],
        [0.12500000, 0.64399999, 0.76899999, 0.74299997, 0.86799997, 0.89399999, 1.01900005, 0.99299997, 1.11800003, 0.24313726],
        [0.09900000, 0.15099999, 0.25000000, 0.25000000, 0.34900001, 0.54400003, 0.64300001, 0.64300001, 0.74199998, 0.16078432],
        [0.09900000, 0.29400000, 0.39300001, 0.39300001, 0.49200001, 0.85399997, 0.95300001, 0.94900000, 1.04799998, 0.11372549],
        [0.09900000, 0.46100000, 0.56000000, 0.55599999, 0.65499997, 1.73000002, 1.82900000, 1.84599996, 1.94500005, 0.16470589],
        [0.09500000, 1.17400002, 1.26900005, 1.28999996, 1.38499999, 2.78600001, 2.88100004, 2.87800002, 2.97300005, 0.20784314],
        [0.11600000, 1.49600005, 1.61199999, 1.58800006, 1.70400000, 2.14000010, 2.25600004, 2.19799995, 2.31399989, 0.24313726],
        [0.09200000, 0.55199999, 0.64399999, 0.61000001, 0.70200002, 1.07799995, 1.16999996, 1.17999995, 1.27199996, 0.13725491],
        [0.05800000, 0.46799999, 0.52600002, 0.56999999, 0.62800002, 0.85200000, 0.91000003, 0.95200002, 1.00999999, 0.26274511],
        [0.10200000, 0.28200001, 0.38400000, 0.38200000, 0.48400000, 0.52499998, 0.62699997, 0.62500000, 0.72700000, 0.14117648],
        [0.10000000, 0.14300001, 0.24300000, 0.24300000, 0.34299999, 1.17900002, 1.27900004, 1.27800000, 1.37800002, 0.16862746],
        [0.10000000, 0.93599999, 1.03600001, 1.03499997, 1.13499999, 1.27800000, 1.37800002, 1.39499998, 1.49500000, 0.19215687],
        [0.09900000, 0.24300000, 0.34200001, 0.36000001, 0.45899999, 1.85599995, 1.95500004, 1.94700003, 2.04600000, 0.18431373],
        [0.11700000, 1.49600005, 1.61300004, 1.58700001, 1.70400000, 1.96399999, 2.08100009, 2.06299996, 2.18000007, 0.12941177],
        [0.09100000, 0.37700000, 0.46799999, 0.47600001, 0.56699997, 0.74400002, 0.83499998, 0.86799997, 0.95899999, 0.24313726],
        [0.09900000, 0.26800001, 0.36700001, 0.39199999, 0.49100000, 0.78600001, 0.88499999, 0.89499998, 0.99400002, 0.16862746],
        [0.12400000, 0.39399999, 0.51800001, 0.50300002, 0.62699997, 1.36399996, 1.48800004, 1.47099996, 1.59500003, 0.16470589],
        [0.10900000, 0.86100000, 0.97000003, 0.96799999, 1.07700002, 1.14400005, 1.25300002, 1.24399996, 1.35300004, 0.24313726],
        [0.10700000, 0.17600000, 0.28299999, 0.27599999, 0.38299999, 0.72799999, 0.83499998, 0.85200000, 0.95899999, 0.18823530],
        [0.10000000, 0.45199999, 0.55199999, 0.57599998, 0.67600000, 0.97000003, 1.07000005, 1.05999994, 1.15999997, 0.14117648],
        [0.12400000, 0.39399999, 0.51800001, 0.48400000, 0.60799998, 0.98699999, 1.11099994, 1.09500003, 1.21899998, 0.12941177],
        [0.09000000, 0.50300002, 0.59299999, 0.61100000, 0.70099998, 0.88700002, 0.97700000, 1.00300002, 1.09300005, 0.24313726],
        [0.10800000, 0.27599999, 0.38400000, 0.39199999, 0.50000000, 0.86000001, 0.96799999, 0.94300002, 1.05100000, 0.15686275],
        [0.11600000, 0.46799999, 0.58399999, 0.55100000, 0.66700000, 0.76899999, 0.88499999, 0.86100000, 0.97700000, 0.12941177],
        [0.08300000, 0.21799999, 0.30100000, 0.31000000, 0.39300001, 0.56900001, 0.65200001, 0.66900003, 0.75199997, 0.13333334],
        [0.09200000, 0.25900000, 0.35100001, 0.35900000, 0.45100001, 0.47600001, 0.56800002, 0.58499998, 0.67699999, 0.18823530],
        [0.10000000, 0.11700000, 0.21699999, 0.22600000, 0.32600001, 0.32699999, 0.42699999, 0.42600000, 0.52600002, 0.24313726],
        [0.10900000, 0.10100000, 0.20999999, 0.20000000, 0.30899999, 2.28900003, 2.39800000, 2.38899994, 2.49799991, 0.11372549],
        [0.09900000, 2.08899999, 2.18799996, 2.18899989, 2.28800011, 2.46499991, 2.56399989, 2.56399989, 2.66300011, 0.16470589],
        [0.10000000, 0.27599999, 0.37599999, 0.37500000, 0.47499999, 0.56000000, 0.66000003, 0.67600000, 0.77600002, 0.24313726],
        [0.09900000, 0.18500000, 0.28400001, 0.30100000, 0.40000001, 0.60299999, 0.70200002, 0.71899998, 0.81800002, 0.18039216],
        [0.11600000, 0.30199999, 0.41800001, 0.41800001, 0.53399998, 0.60299999, 0.71899998, 0.71100003, 0.82700002, 0.14509805],
        [0.11600000, 0.18500000, 0.30100000, 0.29300001, 0.40900001, 0.43500000, 0.55100000, 0.53399998, 0.64999998, 0.13725491],
        [0.10800000, 0.14200000, 0.25000000, 0.24100000, 0.34900001, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.14117648],
        [0.09900000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.18823530]
    ]]
    
    # テストデータをTensorに変換
    test_tensor = torch.tensor(test_data, dtype=torch.float32)
    
    # モデルを評価モードに設定
    model.eval()
    
    print(f"入力データの形状: {test_tensor.shape}")
    
    # 推論実行
    with torch.no_grad():
        try:
            output = model(test_tensor)
            print(f"出力の形状: {output.shape}")
            print(f"出力結果:\n{output}")
        except Exception as e:
            print(f"実行時エラー: {str(e)}")

# テスト実行
print("実データでのテストを開始します...")
evaluate_with_real_data()

